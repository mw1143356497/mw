一、环境安装
    1.安装jdk
        登录官网：https://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html
        下载安装包，根据自身系统选择安装包
        下载完成，运行安装程序
        配置环境变量
            右击“我的电脑”-点击“属性”-“高级系统设置”-“环境变量”，
            在系统变量中新建JAVA_HOME变量，输入你的jdk安装目录，例：D:\JDK
            在系统变量中新建CLASSPATH变量，输入“.;%JAVA_HOME%\lib\dt.jar;%JAVA_HOME%\lib\tools.jar;”
            然后在path中添加“;%JAVA_HOME%\bin;%JAVA_HOME%\jre\bin”
            打开cmd窗口，输入java -version命令，如出现java版本信息则安装成功。
    2.安装IDEA
        登录官网，下载相应版本安装包
        运行安装程序，选择路径、系统位数等，点击安装
        激活码上网搜索lanyu，注意：使用激活码前，修改hosts文件
    3.安装scala插件
        下载对应插件版本
        将下载文件放到idea目录下的plugins目录下
        打开idea，点击左上角苹果图标旁的inteliij idea–>preferences–>plugins
        点击install plugin from disk，选择刚刚放入在idea安装目录下的plugins文件夹里的文件
    4.安装scala
        注：本公司的spark版本为2.3.0，对应的scala版本为2.11.8
        在网站https://www.scala-lang.org/download/ 下载对应版本压缩包
        在本地解压scala压缩包
        在IDEA中依次点击File-Project Settings-Libraries,点击“+”号，选择
        Scala SDK后点击Browser出现选择框后，导入解压好的scala语言包即可。
    5.测试IDEA的scala功能是否成功配置
        打开idea，新建项目选择scala–>IDEA,点击next
        填写项目名，选择jdk和scala开发sdk
        点击finish，在src下面新建一个scala class
        弹框输入类名，并选择类属性为Object，注意不是Class类型
        写上如下代码，在文件上右键运行
            object HelloWorld {
                def main(args: Array[String]): Unit = {
                    print("Hello World!")
                }
            }
        如果IDEA控制台出现“Hello World!”，则上述操作成功！
    6.安装maven
        下载软件包
        解压到本地文件夹
        配置环境变量
            我的电脑-属性-高级系统设置-环境变量，在系统变量中新建“MAVEN_HOME”,值是maven的安装路径
            在path变量中增加“;%MAVEN_HOME%\bin”
        检查maven是否配置成功
            在cmd中输入“mvn -v”出现版本maven版本信息则成功
        修改本地仓库地址
            在maven安装目录下的conf文件夹下有settings.xml文件，在其中第55行输入“<localRepository>address</localRepository>”
            address为你的本地仓库的路径
    7.创建maven项目并在pom.xml导入hadoop-client相应工具包（工具包自行上网寻找。注：hadoop-client，hadoop-common，hadoop-mapreduce-client-core，hadoop-hdfs，hadoop-mapreduce-client-jobclient，hadoop-mapreduce-client-common的版本必须是2.6.0）
    6.下载hadoop2.6.0-win版本，解压到本地，登陆192.168.200.231:7180,账号与密码均为“admin”，在主页点击     Cluster1的三角符号，选择“查看客户端配置”，下载配置文件。
    9.将配置文件复制并覆盖到hadoop解压目录下的\etc\hadoop下。
    10.在C:\Windows\System32\drivers\etc下找到hosts，在最下方增加
            192.168.200.231 cdhtest01
            192.168.200.232 cdhtest02
            192.168.200.233 cdhtest03
            192.168.200.234 cdhtest04
        其中cdhtest01为master
    11.配置hadoop环境变量
            HADOOP_HOME=hadoop安装路径
            在path里追加“;%HADOOP_HOME%\bin;%HADOOP_HOME%\sbin”
            并在用户变量中新建“HADOOP_USER_NAME”,值是连接hadoop集群时的用户名，可以根据情况更改。
    12.在cmd窗口输入“hadoop -version”出现版本信息则安装成功
    13.安装spark
        解压对应的软件包，并在我的电脑配置相应的环境变量“SPARK_HOME”,并在path中添加";%SPARK_HOME%\bin"
    14.在cmd窗口下输入“spark-shell”,出现结果没有报错则安装成功
    

    


/data/spark/checkpoint-1528531555000.bk


        
